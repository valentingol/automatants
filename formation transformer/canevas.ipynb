{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training\n",
    "\n",
    "Requirements:\n",
    " - python 3.7+\n",
    " - pytorch\n",
    " - transformers (Hugging Face)\n",
    " - datasets (Hugging Face)\n",
    " - tqdm\n",
    " - seaborn\n",
    " - matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cell\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import transformers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig,\n",
    "                          DataCollatorForSeq2Seq, Seq2SeqTrainingArguments,\n",
    "                          Seq2SeqTrainer, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "transformers.logging.set_verbosity_warning()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation english to french with T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with a pretrained model - translation en-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (Opus Books) and split into train and validation\n",
    "translation_ds = load_dataset(\"opus_books\", \"en-fr\")['train']\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "\n",
    "\n",
    "# Transformer\n",
    "\n",
    "\n",
    "# Data collator (that pad the sequences dynamically)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"I like Transformers.\", \"The movie right?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset for T5\n",
    "def preprocess_ds(tokenizer, dataset, input_key, output_key, prefix='',\n",
    "                  max_length=128):\n",
    "    # ...\n",
    "    return input_dataset\n",
    "\n",
    "# ...\n",
    "\n",
    "translation_ds['train'][0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('results/pytorch_model.bin')\n",
    "# transformer.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device_num = 0 if torch.cuda.is_available() else -1\n",
    "pipeline_translation = pipeline(\"translation_en_to_fr\",\n",
    "                                model=transformer,\n",
    "                                tokenizer=tokenizer,\n",
    "                                device=device_num)\n",
    "outputs = pipeline_translation([\"I like Transformers.\", \"The movie right?\"])\n",
    "output_text = [output['translation_text'] for output in outputs]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model from random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base config of the needed architecture\n",
    "\n",
    "# Changes the configuration (optional)\n",
    "\n",
    "# Build the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(new_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = 1\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "input_ids = torch.tensor(translation_ds['train']['input_ids'][seq : seq+1],\n",
    "                         device=device)\n",
    "attention_mask = torch.tensor(translation_ds['train']['attention_mask'][seq : seq+1],\n",
    "                              device=device)\n",
    "output_with_attention = transformer.generate(input_ids,\n",
    "                                             attention_mask=attention_mask,\n",
    "                                             output_attentions=True,\n",
    "                                             return_dict_in_generate=True)\n",
    "input_tokens = [\n",
    "        tokenizer.decode(translation_ds['train']['input_ids'][seq][i],\n",
    "                         skip_special_tokens=True)\n",
    "        for i in range(len(translation_ds['train']['input_ids'][seq]))\n",
    "    ]\n",
    "output_tokens = [\n",
    "        tokenizer.decode(\n",
    "            output_with_attention.sequences[0][i], skip_special_tokens=True\n",
    "        )\n",
    "        for i in range(output_with_attention.sequences[0].shape[0])\n",
    "    ]\n",
    "\n",
    "def draw(data, x, y, ax):\n",
    "    sns.heatmap(\n",
    "        data, xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0,\n",
    "        cbar=True, ax=ax, cmap=\"cool\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_raw = None  # TODO\n",
    "attention = None # TODO\n",
    "\n",
    "# attentions: (output_token, layer, sentence, head, 1, input_token)\n",
    "\n",
    "# ...\n",
    "\n",
    "# attentions: (layer, head, output_token, input_token)\n",
    "print(attention.shape)\n",
    "\n",
    "# mean attention over layers and heads\n",
    "mean_attention = torch.mean(attention, dim=[0, 1])\n",
    "print(mean_attention.shape)\n",
    "\n",
    "normalized_mean_attention = mean_attention / torch.max(mean_attention)\n",
    "\n",
    "# plot the heatmap of the mean attention\n",
    "_, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "draw(\n",
    "    normalized_mean_attention.detach().cpu().numpy(),\n",
    "    input_tokens,\n",
    "    output_tokens,\n",
    "    ax=ax,\n",
    "    )\n",
    "plt.savefig(\"ressources/cross_attention.png\", facecolor='white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_raw = output_with_attention.decoder_attentions\n",
    "n = len(attention_raw)\n",
    "\n",
    "attention = torch.zeros((n, n), device=device)\n",
    "for i in range(n):\n",
    "    att = torch.stack(attention_raw[i])[:, 0, :, 0, :]\n",
    "    mean_att = torch.mean(att, dim=[0, 1])\n",
    "    attention[i, torch.arange(end=i+1)] = mean_att\n",
    "\n",
    "normalized_mean_attention = attention / torch.max(attention)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f65de6452115bb8c9d0bb409162d94f4655b416b599aee1edc20beae30cc2cf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('_mainvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
