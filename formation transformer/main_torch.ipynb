{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Training (Pytorch)\n",
    "\n",
    "Requirements:\n",
    " - python 3.7+\n",
    " - pytorch\n",
    " - transformers (Hugging Face)\n",
    " - datasets (Hugging Face)\n",
    " - tqdm\n",
    " - seaborn\n",
    " - matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cell\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import transformers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from transformers import (AutoModelForSeq2SeqLM, AutoTokenizer, AutoConfig,\n",
    "                          DataCollatorForSeq2Seq, Seq2SeqTrainingArguments,\n",
    "                          Seq2SeqTrainer, pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "transformers.logging.set_verbosity_warning()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Translation english to french with T5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference with a pretrained model - translation en-fr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline = pipeline(\"translation_en_to_fr\", model='t5-small')\n",
    "outputs = base_pipeline([\"I like Transformers.\", \"The movie right?\"])\n",
    "output_text = [output['translation_text'] for output in outputs]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning a pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (Opus Books) and split into train and validation\n",
    "translation_ds = load_dataset(\"opus_books\", \"en-fr\")['train']\n",
    "translation_ds = translation_ds.train_test_split(test_size=0.2)\n",
    "translation_ds['train'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "# Transformer\n",
    "transformer = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "# Data collator (that pad the sequences dynamically)\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer([\"I like Transformers.\", \"The movie right?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-process dataset for T5\n",
    "def preprocess_ds(tokenizer, dataset, input_key, output_key, prefix='',\n",
    "                  max_length=128):\n",
    "    input_txt = [prefix + sample[input_key] for sample in dataset['translation']]\n",
    "    output_txt = [sample[output_key] for sample in dataset['translation']]\n",
    "    input_dataset = tokenizer(input_txt, max_length=max_length, truncation=True)\n",
    "    with tokenizer.as_target_tokenizer():  # use output tokenizer here\n",
    "        labels = tokenizer(output_txt, max_length=max_length, truncation=True)\n",
    "    input_dataset['labels'] = labels['input_ids']\n",
    "    return input_dataset\n",
    "\n",
    "translation_ds = translation_ds.map(lambda ds: preprocess_ds(\n",
    "    tokenizer, ds, 'en', 'fr', 'translate English to French: '\n",
    "    ),batched=True)  # process in batch of size 1000\n",
    "\n",
    "translation_ds['train'][0].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_strategy=\"steps\",\n",
    "        logging_steps=200,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        weight_decay=0.01,\n",
    "        save_total_limit=3,\n",
    "        num_train_epochs=1,\n",
    "        warmup_ratio=0.1,\n",
    "        fp16=True,\n",
    "        report_to=\"none\",\n",
    "        )\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=translation_ds[\"train\"],\n",
    "        eval_dataset=translation_ds[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        )\n",
    "\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint = torch.load('results/pytorch_model.bin')\n",
    "# transformer.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0 if torch.cuda.is_available() else -1\n",
    "pipeline_translation = pipeline(\"translation_en_to_fr\",\n",
    "                                model=transformer,\n",
    "                                tokenizer=tokenizer,\n",
    "                                device=device)\n",
    "outputs = pipeline_translation([\"I like Transformers.\", \"The movie right?\"])\n",
    "output_text = [output['translation_text'] for output in outputs]\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model from random initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get base config of the needed architecture\n",
    "config = AutoConfig.from_pretrained(\"t5-small\")\n",
    "# Changes the configuration (optional)\n",
    "config.d_ff = 1024\n",
    "# Build the model\n",
    "new_transformer = AutoModelForSeq2SeqLM.from_config(config)\n",
    "print(new_transformer.config.d_ff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(new_transformer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "input_ids = torch.tensor(translation_ds['train']['input_ids'][1:2], device=device)\n",
    "attention_mask = torch.tensor(translation_ds['train']['attention_mask'][1:2], device=device)\n",
    "output_with_attention = transformer.generate(input_ids,\n",
    "                                             attention_mask=attention_mask,\n",
    "                                             output_attentions=True,\n",
    "                                             return_dict_in_generate=True)\n",
    "attention_raw = output_with_attention.cross_attentions\n",
    "\n",
    "# attention_raw: (output_token, layer, sentence, head, 1, input_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = torch.stack([torch.stack(attention_raw[i], dim=0)\n",
    "                         for i in range(len(attention_raw))])\n",
    "# attentions: (output_token, layer, sentence, head, 1, input_token)\n",
    "attention = torch.squeeze(attention, dim=-2)\n",
    "attention = torch.permute(attention, (2, 1, 3, 0, 4))\n",
    "attention = attention[0]\n",
    "# attentions: (layer, head, output_token, input_token)\n",
    "print(attention.shape)\n",
    "\n",
    "# mean attention over layers and heads\n",
    "mean_attention = torch.mean(attention, dim=[0, 1])\n",
    "print(mean_attention.shape)\n",
    "\n",
    "normalized_mean_attention = mean_attention / torch.max(mean_attention)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw(data, x, y, ax):\n",
    "    sns.heatmap(\n",
    "        data, xticklabels=x, square=True, yticklabels=y, vmin=0.0, vmax=1.0,\n",
    "        cbar=True, ax=ax, cmap=\"cool\"\n",
    "    )\n",
    "\n",
    "input_tokens = [\n",
    "        tokenizer.decode(translation_ds['train']['input_ids'][1][i],\n",
    "                         skip_special_tokens=True)\n",
    "        for i in range(len(translation_ds['train']['input_ids'][1]))\n",
    "    ]\n",
    "output_tokens = [\n",
    "        tokenizer.decode(\n",
    "            output_with_attention.sequences[0][i], skip_special_tokens=True\n",
    "        )\n",
    "        for i in range(output_with_attention.sequences[0].shape[0])\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "draw(\n",
    "    normalized_mean_attention.detach().cpu().numpy(),\n",
    "    input_tokens,\n",
    "    output_tokens,\n",
    "    ax=ax,\n",
    "    )\n",
    "plt.savefig(\"ressources/cross_attention.png\", facecolor='white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_raw = output_with_attention.encoder_attentions\n",
    "attention = torch.stack(attention_raw)\n",
    "attention = torch.squeeze(attention, dim=1)\n",
    "mean_attention = torch.mean(attention, dim=[0, 1])\n",
    "normalized_mean_attention = mean_attention / torch.max(mean_attention)\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "draw(\n",
    "    normalized_mean_attention.detach().cpu().numpy(),\n",
    "    input_tokens,\n",
    "    input_tokens,\n",
    "    ax=ax,\n",
    "    )\n",
    "plt.savefig(\"ressources/input_attention.png\", facecolor='white')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_raw = output_with_attention.decoder_attentions\n",
    "n = len(attention_raw)\n",
    "\n",
    "attention = torch.zeros((n, n), device=device)\n",
    "for i in range(n):\n",
    "    att = torch.stack(attention_raw[i])[:, 0, :, 0, :]\n",
    "    mean_att = torch.mean(att, dim=[0, 1])\n",
    "    attention[i, torch.arange(end=i+1)] = mean_att\n",
    "\n",
    "normalized_mean_attention = attention / torch.max(attention)\n",
    "\n",
    "_, ax = plt.subplots(1, 1, figsize=(15, 15))\n",
    "draw(\n",
    "    normalized_mean_attention.detach().cpu().numpy(),\n",
    "    output_tokens,\n",
    "    output_tokens,\n",
    "    ax=ax,\n",
    "    )\n",
    "plt.savefig(\"ressources/output_attention.png\", facecolor='white')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f65de6452115bb8c9d0bb409162d94f4655b416b599aee1edc20beae30cc2cf3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('_mainvenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
